# backend/sms/enhanced_report_service.py - Versi√≥n con Debug Mejorado
import base64
import io
import re
import os
from datetime import datetime
from collections import Counter
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image as RLImage, ListFlowable, ListItem
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER
from django.conf import settings

# Importaciones condicionales para OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
    print("üì¶ OpenAI library imported successfully")
except ImportError as e:
    OPENAI_AVAILABLE = False
    print(f"‚ö†Ô∏è OpenAI not installed: {e}")

from .semantic_analysis import SemanticResearchAnalyzer

class EnhancedReportGeneratorService:
    """
    Servicio mejorado para generar reportes metodol√≥gicos completos
    integrando las visualizaciones existentes del sistema y an√°lisis con IA.
    """
    
    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
        
        # Configurar OpenAI con debugging mejorado
        self.client = None
        self.openai_status = self._setup_openai_client()
        
    def _setup_openai_client(self):
        """Configurar cliente OpenAI con debugging detallado"""
        print("üîç Iniciando configuraci√≥n de OpenAI...")
        
        if not OPENAI_AVAILABLE:
            print("‚ùå OpenAI library no disponible")
            return "library_not_available"
        
        # M√©todo 1: Desde settings de Django
        api_key = getattr(settings, 'OPENAI_API_KEY', '')
        if api_key:
            print(f"‚úÖ Clave encontrada en Django settings (longitud: {len(api_key)})")
        else:
            print("‚ö†Ô∏è No se encontr√≥ clave en Django settings")
            
            # M√©todo 2: Directamente desde variables de entorno
            api_key = os.environ.get('OPENAI_API_KEY', '')
            if api_key:
                print(f"‚úÖ Clave encontrada en variables de entorno (longitud: {len(api_key)})")
            else:
                print("‚ùå No se encontr√≥ clave en variables de entorno")
        
        # M√©todo 3: Desde archivo .env (manual)
        if not api_key:
            try:
                from dotenv import load_dotenv
                load_dotenv()
                api_key = os.environ.get('OPENAI_API_KEY', '')
                if api_key:
                    print(f"‚úÖ Clave encontrada despu√©s de cargar .env (longitud: {len(api_key)})")
                else:
                    print("‚ùå No se encontr√≥ clave despu√©s de cargar .env")
            except ImportError:
                print("‚ö†Ô∏è python-dotenv no instalado")
        
        if not api_key:
            print("‚ùå No se encontr√≥ clave API de OpenAI en ning√∫n m√©todo")
            return "no_api_key"
        
        # Validar formato de la clave
        if not api_key.startswith('sk-'):
            print(f"‚ö†Ô∏è Formato de clave inv√°lido. Debe comenzar con 'sk-', actual: {api_key[:10]}...")
            return "invalid_key_format"
        
        if len(api_key) < 40:
            print(f"‚ö†Ô∏è Clave parece incompleta. Longitud: {len(api_key)}, esperada: ~51")
            return "key_too_short"
        
        # Intentar crear el cliente
        try:
            self.client = OpenAI(api_key=api_key)
            print("‚úÖ Cliente OpenAI creado exitosamente")
            
            # Test b√°sico de conectividad
            try:
                test_response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=5
                )
                print("‚úÖ Test de conectividad OpenAI exitoso")
                return "configured_and_tested"
            except Exception as test_error:
                print(f"‚ö†Ô∏è Cliente creado pero test fall√≥: {test_error}")
                return "client_created_but_test_failed"
                
        except Exception as e:
            print(f"‚ùå Error creando cliente OpenAI: {e}")
            self.client = None
            return f"client_creation_failed: {str(e)}"
    
    def _setup_custom_styles(self):
        """Configurar estilos personalizados para el documento"""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Heading1'],
            fontSize=16,
            spaceAfter=12,
            textColor=colors.HexColor('#2563eb'),
            alignment=TA_CENTER
        ))
        
        self.styles.add(ParagraphStyle(
            name='CustomHeading2',
            parent=self.styles['Heading2'],
            fontSize=14,
            spaceAfter=10,
            textColor=colors.HexColor('#1f2937')
        ))
        
        self.styles.add(ParagraphStyle(
            name='CustomNormal',
            parent=self.styles['Normal'],
            fontSize=11,
            alignment=TA_JUSTIFY,
            spaceAfter=12
        ))
        
        # NUEVO ESTILO para encabezados de tabla
        self.styles.add(ParagraphStyle(
            name='CustomHeading5',
            parent=self.styles['Normal'],
            fontSize=10,
            fontName='Helvetica-Bold',
            textColor=colors.black,
            alignment=TA_LEFT,
            spaceAfter=0,
            spaceBefore=0
        ))
    
    def generate_comprehensive_report(self, sms_data, articles_data, visualizations_data=None):
        """
        Genera un reporte metodol√≥gico completo incluyendo visualizaciones
        
        Args:
            sms_data: Datos del SMS
            articles_data: Lista de art√≠culos procesados
            visualizations_data: Datos de las visualizaciones (opcional)
        """
        print(f"üìÑ Generando reporte completo. Estado OpenAI: {self.openai_status}")
        
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, topMargin=inch, bottomMargin=inch)
        
        story = []
        
        # 1. Portada y Abstract
        story.extend(self._generate_title_and_abstract(sms_data))
        
        # 2. Introducci√≥n con citas
        story.extend(self._generate_introduction(sms_data, articles_data))
        
        # 3. Metodolog√≠a detallada (A. Planning + c)
        story.extend(self._generate_detailed_methodology(sms_data, articles_data, visualizations_data))
        
        # 4. NUEVA SECCI√ìN: C. Results mejorada con IA
        story.extend(self._generate_comprehensive_results_section(sms_data, articles_data, visualizations_data))

        # 5. Tabla de extracci√≥n de informaci√≥n (NUEVA POSICI√ìN)
        story.extend(self._generate_information_extraction_table(articles_data))
        
        # 6. NUEVA SECCI√ìN: Analysis and discussions (AGREGAR AQU√ç)
        story.extend(self._generate_analysis_and_discussions_section(sms_data, articles_data, visualizations_data))

        # 6. Visualizaciones integradas
        if visualizations_data:
            story.extend(self._integrate_visualizations(visualizations_data, sms_data))
        
        # 7. Conclusiones
        story.extend(self._generate_conclusions(sms_data, articles_data))
        
        # 8. Referencias bibliogr√°ficas
        story.extend(self._generate_references(articles_data))

        doc.build(story)
        buffer.seek(0)
        return buffer.read()
    
    def _generate_ai_text(self, prompt, max_tokens=500):
        """Generar texto usando OpenAI GPT con fallbacks mejorados"""
        print(f"ü§ñ Generando texto con IA. Cliente disponible: {self.client is not None}")
        
        try:
            if not self.client:
                print(f"‚ö†Ô∏è Cliente no disponible. Status: {self.openai_status}")
                return self._generate_academic_fallback_text(prompt)
            
            print(f"üì° Enviando prompt a OpenAI (longitud: {len(prompt)})")
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are an expert academic writer specializing in systematic reviews."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            generated_text = response.choices[0].message.content.strip()
            print(f"‚úÖ Texto generado exitosamente (longitud: {len(generated_text)})")
            return generated_text
            
        except Exception as e:
            print(f"‚ùå Error en generaci√≥n con IA: {e}")
            return self._generate_academic_fallback_text(prompt)
    
    def _generate_academic_fallback_text(self, prompt):
        """Generar texto acad√©mico profesional como fallback"""
        prompt_lower = prompt.lower()
        
        if "abstract" in prompt_lower:
            return 
        
        elif "introduction" in prompt_lower:
            return 
        
        elif "conclusion" in prompt_lower:
            return 
        
        elif "rsq" in prompt_lower or "research question" in prompt_lower:
            return 
    
    def _generate_title_and_abstract(self, sms_data):
        """Generar t√≠tulo y abstract del documento"""
        story = []
        
        # T√≠tulo principal
        story.append(Paragraph(
            f"{sms_data['titulo_estudio']}: A Systematic Mapping Study", 
            self.styles['CustomTitle']
        ))
        story.append(Spacer(1, 20))
        
        # Autores
        story.append(Paragraph(f"Authors: {sms_data['autores']}", self.styles['Normal']))
        story.append(Spacer(1, 10))
        
        # Abstract generado con IA
        abstract = self._generate_ai_text(
            f"Escribe un abstract para una revisi√≥n sistem√°tica sobre {sms_data['titulo_estudio']} en ingles. "
            f"Incluya los objetivos de la investigaci√≥n, la metodolog√≠a y las principales conclusiones en estilo acad√©mico."
            f"solo dame el texto sin titulo ni listas "
        )
        
        story.append(Paragraph("Abstract", self.styles['CustomHeading2']))
        story.append(Paragraph(abstract, self.styles['CustomNormal']))
        
        # Keywords
        keywords = self._extract_keywords_from_title(sms_data['titulo_estudio'])
        story.append(Paragraph(f"Keywords: {keywords}", self.styles['CustomNormal']))
        story.append(Spacer(1, 30))
        
        return story
    
    def _generate_introduction(self, sms_data, articles_data):
        """Generar una introducci√≥n extensa, coherente y profunda, sin gerundios, usando todos los datos del sistema."""
        story = []
        story.append(Paragraph("INTRODUCCI√ìN", self.styles['CustomHeading2']))

        # Estad√≠sticas reales del sistema
        total_articles = len(articles_data)
        years = [a.get('anio_publicacion', 'Desconocido') for a in articles_data]
        year_stats = {y: years.count(y) for y in set(years)}
        top_years = sorted(year_stats.items(), key=lambda x: x[1], reverse=True)[:3]
        top_authors = {}
        for a in articles_data:
            for author in a.get('autores', '').split(','):
                author = author.strip()
                if author:
                    top_authors[author] = top_authors.get(author, 0) + 1
        top_authors_list = sorted(top_authors.items(), key=lambda x: x[1], reverse=True)[:3]
        topics = []
        for a in articles_data:
            if a.get('palabras_clave'):
                topics.extend([t.strip() for t in a['palabras_clave'].split(',')])
        topic_stats = {}
        for t in topics:
            if t:
                topic_stats[t] = topic_stats.get(t, 0) + 1
        top_topics = sorted(topic_stats.items(), key=lambda x: x[1], reverse=True)[:5]

        stats_text = (
            f"En el an√°lisis de la literatura, se identificaron {total_articles} art√≠culos relevantes. "
            f"Los a√±os con mayor producci√≥n cient√≠fica fueron: {', '.join([f'{y} ({c} art√≠culos)' for y, c in top_years])}. "
            f"Entre los autores m√°s prol√≠ficos se encuentran: {', '.join([f'{a} ({c} art√≠culos)' for a, c in top_authors_list])}. "
            f"Los temas m√°s frecuentes son: {', '.join([f'{t} ({c} menciones)' for t, c in top_topics])}."
        )

        # 1. Contexto y motivaci√≥n
        context_prompt = (
            f"Redacta la primera parte de una introducci√≥n acad√©mica extensa (al menos 700 palabras) para un mapeo sistem√°tico titulado '{sms_data['titulo_estudio']}'. "
            f"Describe el contexto general del √°rea, su evoluci√≥n, importancia y motivaci√≥n para investigar este tema. "
            f"Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad. Utiliza datos reales cuando sea posible. "
            f"{stats_text}"
            f"No incluyas ning√∫n t√≠tulo, encabezado ni la palabra 'Introducci√≥n' al inicio. Solo el texto corrido"
        )
        context_text = self._generate_ai_text(context_prompt, max_tokens=1000)
        story.append(Paragraph(context_text, self.styles['CustomNormal']))

        # 2. Estado del arte y tendencias
        state_of_art_prompt = (
            f"Redacta una secci√≥n de introducci√≥n (al menos 700 palabras) que describa el estado del arte sobre '{sms_data['titulo_estudio']}', "
            f"incluyendo tendencias, vac√≠os y temas recurrentes. Utiliza los siguientes datos: {stats_text}. "
            f"Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad."
            f"solo dame el texto sin titulo ni listas "
        )
        state_of_art_text = self._generate_ai_text(state_of_art_prompt, max_tokens=1000)
        story.append(Paragraph(state_of_art_text, self.styles['CustomNormal']))

        # 3. Vac√≠os y justificaci√≥n
        gaps_prompt = (
            f"Redacta una secci√≥n (al menos 500 palabras) que identifique vac√≠os en la literatura y justifique la necesidad de realizar el mapeo sistem√°tico sobre '{sms_data['titulo_estudio']}'. "
            f"Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad. Utiliza datos reales cuando sea posible."
            f"solo dame el texto sin titulo ni listas "
        )
        gaps_text = self._generate_ai_text(gaps_prompt, max_tokens=1000)
        story.append(Paragraph(gaps_text, self.styles['CustomNormal']))

        # 4. Objetivos y relevancia del SMS
        objectives_prompt = (
            f"Redacta una secci√≥n (al menos 500 palabras) que exponga los objetivos del estudio, la relevancia del mapeo sistem√°tico y c√≥mo contribuye al √°rea de '{sms_data['titulo_estudio']}'. "
            f"Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad. Utiliza datos reales cuando sea posible."
            f"solo dame el texto sin titulo ni listas "
        )
        objectives_text = self._generate_ai_text(objectives_prompt, max_tokens=1000)
        story.append(Paragraph(objectives_text, self.styles['CustomNormal']))

        # 5. Cierre de la introducci√≥n
        closing_prompt = (
            f"Redacta el cierre de la introducci√≥n (al menos 500 palabras), resumiendo la importancia del estudio y anticipando la estructura del documento. "
            f"Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad."
            f"solo dame el texto sin titulo ni listas "
        )
        closing_text = self._generate_ai_text(closing_prompt, max_tokens=1000)
        story.append(Paragraph(closing_text, self.styles['CustomNormal']))

        return story

    
    def _generate_detailed_methodology(self, sms_data, articles_data, visualizations_data=None):
        """Generar secci√≥n de metodolog√≠a detallada"""
        story = []
        
        story.append(Paragraph("1. MATERIALES Y M√âTODOS", self.styles['CustomHeading2']))
        
        # Introducci√≥n a la metodolog√≠a
        story.append(Paragraph(
            f"""
            Este mapeo sistematico (SMS) se basa en las directrices 
            propuestas en (Kitchenham et al, 2010) & (Petersen et al, 2015), 
            cuyo principal objetivo es analizar el estado actual de la 
            investigaci√≥n relacionada con las aplicaciones inform√°ticas 
            para {sms_data['titulo_estudio']}. Esta revisi√≥n consta de dos 
            etapas: planificaci√≥n y ejecuci√≥n. 
            La primera aborda la definici√≥n de las preguntas de investigaci√≥n,
            especificando la intervenci√≥n de inter√©s, 
            el proceso de b√∫squeda y la definici√≥n de los criterios de selecci√≥n
            de art√≠culos. La segunda etapa implementa 
            el proceso de selecci√≥n de las investigaciones relevantes para el objeto
            de estudio, mediante 
            la aplicaci√≥n de los criterios de selecci√≥n y la extracci√≥n de datos para
            obtener los resultados de esta revisi√≥n sistem√°tica.
            """,
            self.styles['CustomNormal']
        ))
        
        # Subsecci√≥n A: Planificaci√≥n
        story.append(Paragraph("A. Planning", self.styles['Heading3']))
        
        story.append(Paragraph(
            f"""
            La intenci√≥n de esta investigaci√≥n se regir√° por la pregunta 
            principal de investigaci√≥n (PRQ): 
            {sms_data.get('pregunta_principal', 'Not defined')}""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            La PRQ busca localizar documentos relevantes sobre el tema 
            propuesto, para lograr este objetivo se divide en tres subpreguntas 
            de investigaci√≥n (RSQ). """,
            self.styles['CustomNormal']
        ))
        
        # Sub-preguntas
        for i, key in enumerate(['subpregunta_1', 'subpregunta_2', 'subpregunta_3'], 1):
            if sms_data.get(key):
                story.append(Paragraph(
                    f"-RSQ_{i}: {sms_data[key]}", 
                    self.styles['CustomNormal']
                ))
        
        story.append(Paragraph(
            f"""
            Para llevar a cabo la b√∫squeda de publicaciones cient√≠ficas que contribuyan 
            al an√°lisis del objeto de estudio, se consideran tres bases
            de datos de citas: Scopus, Web of Science (WoS) y PubMed. Las dos 
            primeras son bases de datos bibliogr√°ficas de res√∫menes y citas de 
            art√≠culos de revistas cient√≠ficas. La tercera es un motor de b√∫squeda de 
            referencias bibliogr√°ficas. Todas estas bases de 
            datos se complementan entre s√≠, ya que incluyen documentos de 
            diversas fuentes, incluyendo art√≠culos de revistas y ponencias de 
            congresos.""",
            self.styles['CustomNormal']
        ))

        # Funci√≥n para convertir n√∫meros a n√∫meros romanos
        def to_roman(num):
            values = [5, 4, 1]
            symbols = ["V", "IV", "I"]
            roman = ""
            for i in range(len(values)):
                count = num // values[i]
                roman += symbols[i] * count
                num -= values[i] * count
            return roman

        keywords = self._extract_keywords_from_title(sms_data['titulo_estudio'])
        keywords_list = keywords.split(', ')

        # Generar enumeraci√≥n con n√∫meros romanos
        keywords_enum = ', '.join([f"{to_roman(i+1)}) {kw}" for i, kw in enumerate(keywords_list)])

        story.append(Paragraph(
            f"""
            El proceso de revisi√≥n consiste en la elecci√≥n de palabras clave, las 
            cuales surgen de la pregunta de investigaci√≥n: {keywords_enum}.""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Las palabras clave permiten identificar sin√≥nimos y t√©rminos 
            relacionados con el objeto de estudio, que al combinarse forman 
            la cadena de b√∫squeda, cuyo prop√≥sito es identificar art√≠culos 
            relevantes para la investigaci√≥n. El per√≠odo de b√∫squeda de 
            publicaciones relevantes se realiz√≥ entre {sms_data.get('anio_inicio', 'N/A')} 
            y {sms_data.get('anio_final', 'N/A')}, debido a la 
            velocidad con la que se producen los cambios tecnol√≥gicos.""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Una vez encontrados los documentos, se aplicaron criterios de inclusi√≥n y 
            exclusi√≥n para la preselecci√≥n y selecci√≥n de los art√≠culos relevantes. Los 
            principales criterios de una revisi√≥n sistem√°tica son los siguientes:""",
            self.styles['CustomNormal']
        ))
        
        # Criterios de inclusi√≥n/exclusi√≥n
        story.append(Paragraph("Criterios de inclusi√≥n:", self.styles['Heading5']))
        inclusion = sms_data.get('criterios_inclusion')
        if inclusion:
            # Si es string, convi√©rtelo a lista
            if isinstance(inclusion, str):
                criterios_inclusion = [c.strip() for c in inclusion.split('\n') if c.strip()]
            else:
                criterios_inclusion = inclusion
            story.append(ListFlowable(
                [ListItem(Paragraph(criterio, self.styles['CustomNormal'])) for criterio in criterios_inclusion],
                bulletType='bullet'
            ))

        story.append(Paragraph("Criterios de exclusi√≥n:", self.styles['Heading5']))
        exclusion = sms_data.get('criterios_exclusion')
        if exclusion:
            if isinstance(exclusion, str):
                criterios_exclusion = [c.strip() for c in exclusion.split('\n') if c.strip()]
            else:
                criterios_exclusion = exclusion
            story.append(ListFlowable(
                [ListItem(Paragraph(criterio, self.styles['CustomNormal'])) for criterio in criterios_exclusion],
                bulletType='bullet'
            ))
        
        # Subsecci√≥n B: Ejecuci√≥n
        story.append(Paragraph("B. Execute", self.styles['Heading3']))
        
        story.append(Paragraph(
            f"""
            El proceso de ejecuci√≥n comienza con la aplicaci√≥n de la cadena de 
            b√∫squeda inicial en bases de datos indexadas con el fin de refinar la cadena 
            y encontrar los art√≠culos relevantes al objeto de estudio.""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Durante la primera iteraci√≥n de b√∫squeda en las bases de datos bibliogr√°ficas 
            seleccionadas, el sistema obtuvo autom√°ticamente el n√∫mero de publicaciones 
            encontradas en cada fuente, de acuerdo con los criterios y el periodo 
            definidos por el usuario. Esta informaci√≥n inicial proporciona una visi√≥n 
            general del volumen de literatura disponible y constituye el punto de partida 
            para el proceso de selecci√≥n y an√°lisis de los art√≠culos relevantes.""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Despu√©s de una serie de pruebas y revisiones, se identificaron t√©rminos 
            relacionados y sus sin√≥nimos, de tal manera que: {keywords_enum}.""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Por lo tanto, el modelo est√°ndar de cadenas de b√∫squeda se expresa 
            de la siguiente manera: {sms_data.get('cadena_busqueda', 'Not specified')}""",
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Una vez aplicada la cadena de b√∫squeda refinada, 
            el sistema identific√≥ autom√°ticamente los estudios 
            primarios relevantes en cada base de datos seleccionada. 
            Para maximizar la exhaustividad de la revisi√≥n, tambi√©n 
            se consideraron las referencias de los estudios encontrados, 
            empleando la t√©cnica de b√∫squeda en "bola de nieve" 
            (Wohlin, 2014), con el objetivo de localizar art√≠culos 
            adicionales pertinentes al objeto de estudio.
            """,
            self.styles['CustomNormal']
        ))
        
        # Obtener datos PRISMA
        analyzer = SemanticResearchAnalyzer()
        real_data = analyzer._extract_real_prisma_data(articles_data, sms_data)
        
        story.append(Paragraph(
            f"""
            Tal como se ilustra en la Figura 1, el sistema documenta detalladamente 
            el proceso de selecci√≥n de art√≠culos. Inicialmente, se identificaron 
            {real_data['initial_search']} estudios potencialmente relevantes a trav√©s 
            de las bases de datos seleccionadas, y se identificaron {real_data['additional_sources']}  
            estudios adicionales a trav√©s de otras fuentes.  Despu√©s de eliminar {real_data['duplicates_removed']} 
            art√≠culo duplicado, quedaron {real_data['after_duplicates']} art√≠culos para la revisi√≥n de t√≠tulo 
            y resumen. De estos, {real_data['title_abstract_excluded']} fueron 
            excluidos bas√°ndose en los criterios de inclusi√≥n y exclusi√≥n. 
            Los {real_data['full_text_assessed']} art√≠culos restantes fueron evaluados a texto completo, 
            de los cuales {real_data['full_text_excluded']} fueron excluidos. Finalmente, {real_data['final_included']} estudios cumplieron 
            con todos los criterios y fueron incluidos en la s√≠ntesis cuantitativa, 
            representando una tasa de selecci√≥n del {real_data['selection_rate']}%.
            """,
            self.styles['CustomNormal']
        ))
        
        story.append(Paragraph(
            f"""
            Finalmente, se obtuvieron {real_data['final_included']} art√≠culos para ser analizados en detalle. 
            Una primera aproximaci√≥n de los estudios relevantes es su pre-selecci√≥n; 
            primero, se descartaron los art√≠culos cuyo idioma era diferente del ingl√©s, 
            y luego se analizaron el t√≠tulo, el resumen y las palabras clave de cada art√≠culo
            para verificar si est√°n relacionados con el objeto de estudio. Despu√©s de esta 
            evaluaci√≥n, se eliminaron {real_data['full_text_excluded']} art√≠culos y se obtuvieron {real_data['final_included']} art√≠culos para su 
            revisi√≥n completa. Para la selecci√≥n de art√≠culos, se analiz√≥ a fondo el texto 
            completo para determinar si el art√≠culo estaba estrechamente relacionado con 
            el objeto de estudio, o si se desarroll√≥ una aplicaci√≥n que pudiera verificar 
            la construcci√≥n de una herramienta como requisito m√≠nimo para la selecci√≥n. 
            Para evitar la subjetividad, las actividades de revisi√≥n de art√≠culos y 
            extracci√≥n de datos se realizaron de manera independiente. 
            Si un art√≠culo no fue incluido, se mencion√≥ el motivo de su exclusi√≥n. 
            Con este an√°lisis, se eliminaron {real_data['full_text_excluded']} 
            art√≠culos, resultando en {real_data['final_included']} estudios 
            utilizados para la extracci√≥n de datos.""",
            self.styles['CustomNormal']
        ))
        
        if visualizations_data and 'prisma_diagram' in visualizations_data:
            
            # A√±adir imagen del diagrama PRISMA
            prisma_image = self._base64_to_reportlab_image(
                visualizations_data['prisma_diagram']['image_base64']
            )
            if prisma_image:
                story.append(prisma_image)
                story.append(Spacer(1, 10))
                story.append(Paragraph(
                    "Figura 1. Diagrama de flujo que muestra una visi√≥n general del proceso "
                    "de investigaci√≥n para estudios relevantes.", 
                    self.styles['CustomNormal']
                ))

        return story
    
    def _generate_comprehensive_results_section(self, sms_data, articles_data, visualizations_data=None):
        """
        Genera la secci√≥n C. Results con estructura acad√©mica completa usando ChatGPT.
        """
        story = []
        print("üìä Generando secci√≥n C. Results con IA...")
        
        # T√≠tulo de la secci√≥n
        story.append(Paragraph("C. Results", self.styles['Heading3']))
        
        # Obtener estad√≠sticas reales para el an√°lisis
        stats = self._extract_detailed_statistics(articles_data)
        print(f"üìà Estad√≠sticas extra√≠das: {stats['selected_count']} seleccionados de {stats['total_articles']}")
        
        # Paso 1: P√°rrafo Introductorio
        intro_paragraph = self._generate_results_introduction(sms_data, stats, visualizations_data)
        story.append(Paragraph(intro_paragraph, self.styles['CustomNormal']))
        story.append(Spacer(1, 15))
        
        # Paso 2: Respuestas Sistem√°ticas a las Preguntas de Investigaci√≥n
        for i in range(1, 4):
            subquestion_key = f'subpregunta_{i}'
            if sms_data.get(subquestion_key):
                print(f"üîç Generando an√°lisis para RSQ_{i}")
                rsq_section = self._generate_rsq_analysis(
                    question_number=i,
                    question_text=sms_data[subquestion_key],
                    articles_data=articles_data,
                    statistics=stats,
                    visualizations_data=visualizations_data
                )
                story.extend(rsq_section)
        
        return story

    def _extract_detailed_statistics(self, articles_data):
        """Extrae estad√≠sticas detalladas de los art√≠culos para el an√°lisis."""
        
        # Estad√≠sticas b√°sicas
        total_articles = len(articles_data)
        selected_articles = [a for a in articles_data if a.get('estado') == 'SELECTED']
        selected_count = len(selected_articles)
        
        # An√°lisis por a√±o
        years = [a.get('anio_publicacion') for a in selected_articles if a.get('anio_publicacion')]
        year_distribution = Counter(years)
        
        # An√°lisis por revista/journal
        journals = [a.get('journal', 'Sin revista') for a in selected_articles 
                    if a.get('journal') and a.get('journal') != 'Sin revista']
        journal_distribution = Counter(journals)
        
        # An√°lisis de respuestas a subpreguntas
        subq1_responses = [a.get('respuesta_subpregunta_1', '') for a in selected_articles 
                           if a.get('respuesta_subpregunta_1') and a.get('respuesta_subpregunta_1') != 'Sin respuesta disponible']
        subq2_responses = [a.get('respuesta_subpregunta_2', '') for a in selected_articles 
                           if a.get('respuesta_subpregunta_2') and a.get('respuesta_subpregunta_2') != 'Sin respuesta disponible']
        subq3_responses = [a.get('respuesta_subpregunta_3', '') for a in selected_articles 
                           if a.get('respuesta_subpregunta_3') and a.get('respuesta_subpregunta_3') != 'Sin respuesta disponible']
        
        print(f"üìã Respuestas encontradas: RSQ1={len(subq1_responses)}, RSQ2={len(subq2_responses)}, RSQ3={len(subq3_responses)}")
        
        # An√°lisis por enfoque (si existe)
        enfoques = [a.get('enfoque', 'No especificado') for a in selected_articles 
                    if a.get('enfoque')]
        enfoque_distribution = Counter(enfoques)
        
        return {
            'total_articles': total_articles,
            'selected_count': selected_count,
            'selection_rate': round((selected_count / total_articles) * 100, 1) if total_articles > 0 else 0,
            'year_distribution': dict(year_distribution.most_common(10)),
            'journal_distribution': dict(journal_distribution.most_common(10)),
            'enfoque_distribution': dict(enfoque_distribution.most_common()),
            'year_range': f"{min(years) if years else 'N/A'} - {max(years) if years else 'N/A'}",
            'most_productive_year': year_distribution.most_common(1)[0] if year_distribution else ('N/A', 0),
            'subq1_responses': subq1_responses,
            'subq2_responses': subq2_responses,
            'subq3_responses': subq3_responses,
            'avg_responses_per_question': round(
                (len(subq1_responses) + len(subq2_responses) + len(subq3_responses)) / 3, 1
            )
        }

    def _generate_results_introduction(self, sms_data, stats, visualizations_data):
        """Genera p√°rrafo introductorio para la secci√≥n de resultados usando ChatGPT."""
        prompt = f"""
        Escribe un p√°rrafo introductorio acad√©mico profesional para la secci√≥n de resultados 
        de un mapeo sistem√°tico sobre "{sms_data['titulo_estudio']}".
        
        Informaci√≥n clave para incluir:
        - Se analizaron {stats['selected_count']} estudios de un total de {stats['total_articles']} identificados
        - Tasa de selecci√≥n del {stats['selection_rate']}%
        - Periodo de publicaci√≥n: {stats['year_range']}
        - Se responder√°n las siguientes preguntas de investigaci√≥n:
          1. {sms_data.get('subpregunta_1', 'Pregunta 1 no definida')}
          2. {sms_data.get('subpregunta_2', 'Pregunta 2 no definida')}
          3. {sms_data.get('subpregunta_3', 'Pregunta 3 no definida')}
        
        El p√°rrafo debe:
        - Establecer el contexto de los hallazgos
        - Mencionar las figuras y tablas que se presentar√°n (Figura 2, Figura 3, Tabla I)
        - Explicar que se utiliz√≥ an√°lisis sem√°ntico y clustering
        - Mantener un tono acad√©mico formal
        - Ser de aproximadamente 100-120 palabras
        Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad.
        solo dame el texto sin titulo ni listas 
        Responde SOLO con el p√°rrafo, sin texto adicional.
        """
        
        return self._generate_ai_text(prompt)

    def _generate_rsq_analysis(self, question_number, question_text, articles_data, statistics, visualizations_data):
        """Genera an√°lisis detallado para una pregunta de investigaci√≥n espec√≠fica usando ChatGPT."""
        story = []
        
        # T√≠tulo de la RSQ
        story.append(Paragraph(f"RSQ_{question_number}: {question_text}", self.styles['Heading3']))
        
        # Obtener respuestas espec√≠ficas para esta pregunta
        subq_key = f'subq{question_number}_responses'
        responses = statistics.get(subq_key, [])
        
        print(f"üîç Analizando RSQ_{question_number} con {len(responses)} respuestas")
        
        # Analizar respuestas para extraer patrones
        analysis_data = self._analyze_subquestion_responses(responses, question_number)
        
        # Generar an√°lisis con ChatGPT
        analysis_prompt = f"""
        Eres un investigador experto analizando resultados de un mapeo sistem√°tico.
        
        Pregunta de investigaci√≥n: {question_text}
        
        Datos para analizar:
        - N√∫mero total de estudios analizados: {len(responses)}
        - Patrones identificados: {analysis_data['patterns']}
        - Categor√≠as principales: {analysis_data['categories']}
        - Frecuencias: {analysis_data['frequencies']}
        
        Estad√≠sticas adicionales:
        - Total de estudios seleccionados: {statistics['selected_count']}
        - Distribuci√≥n por a√±o: {statistics['year_distribution']}
        - Distribuci√≥n por enfoque: {statistics['enfoque_distribution']}
        
        Genera un an√°lisis acad√©mico estructurado que incluya:
        
        1. **Metodolog√≠a de an√°lisis** (1-2 oraciones sobre c√≥mo se analiz√≥)
        2. **Estad√≠sticas cuantitativas** (porcentajes espec√≠ficos y frecuencias)
        3. **An√°lisis detallado** (interpretaci√≥n de patrones encontrados)
        4. **Hallazgos principales** (insights m√°s importantes)
        
        Usa un lenguaje acad√©mico formal y cita porcentajes espec√≠ficos.
        Estructura la respuesta en p√°rrafos claros de 80-200 palabras cada uno.
        Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad.
        solo dame el texto sin titulo ni listas 
        """
        
        analysis_text = self._generate_ai_text(analysis_prompt, max_tokens=800)
        
        # Dividir en p√°rrafos y a√±adir al story
        paragraphs = analysis_text.split('\n\n')
        for paragraph in paragraphs:
            if paragraph.strip():
                # Limpiar marcadores de markdown si existen
                clean_paragraph = paragraph.replace('**', '').replace('*', '').strip()
                story.append(Paragraph(clean_paragraph, self.styles['CustomNormal']))
                story.append(Spacer(1, 8))
        
        return story

    def _analyze_subquestion_responses(self, responses, question_number):
        """Analiza las respuestas a una subpregunta espec√≠fica para extraer patrones."""
        if not responses:
            return {
                'patterns': [],
                'categories': {},
                'frequencies': {},
                'keywords': []
            }
        
        # Combinar todas las respuestas para an√°lisis
        combined_text = ' '.join(responses).lower()
        
        # Extraer palabras clave frecuentes (excluyendo stopwords)
        stop_words = {'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'del', 'los', 'las', 'una', 'su', 'este', 'esta', 'como', 'm√°s', 'pero', 'sus', 'muy', 'sin', 'sobre', 'entre', 'ser', 'estar', 'hacer', 'the', 'of', 'and', 'to', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on', 'are', 'as', 'with', 'his', 'they', 'i', 'at', 'be', 'this', 'have', 'from', 'or', 'one', 'had', 'by', 'word', 'but', 'not', 'what', 'all', 'were', 'we', 'when', 'your', 'can', 'said', 'there', 'each', 'which', 'she', 'do', 'how', 'their', 'if', 'will', 'up', 'other', 'about', 'out', 'many', 'then', 'them', 'these', 'so', 'some', 'her', 'would', 'make', 'like', 'into', 'him', 'has', 'two', 'more', 'much', 'my', 'way', 'been', 'who', 'its', 'now', 'find', 'long', 'down', 'day', 'did', 'get', 'come', 'made', 'may', 'part'}
        
        # Extraer palabras significativas
        words = re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú]{3,}\b', combined_text)
        filtered_words = [word for word in words if word not in stop_words]
        word_freq = Counter(filtered_words)
        
        # Categorizaci√≥n b√°sica seg√∫n el n√∫mero de pregunta
        if question_number == 1:
            # Pregunta 1: Usualmente sobre m√©todos/enfoques
            tech_keywords = ['machine', 'learning', 'algorithm', 'statistical', 'analysis', 'model', 'approach', 'method', 'technique', 'algoritmo', 'an√°lisis', 'm√©todo', 't√©cnica', 'estad√≠stico']
            categories = self._categorize_by_keywords(responses, tech_keywords, 'T√©cnicas y M√©todos')
        elif question_number == 2:
            # Pregunta 2: Usualmente sobre aplicaciones/dominios
            app_keywords = ['health', 'medical', 'clinical', 'diagnostic', 'treatment', 'patient', 'disease', 'salud', 'm√©dico', 'cl√≠nico', 'diagn√≥stico', 'tratamiento', 'paciente', 'enfermedad']
            categories = self._categorize_by_keywords(responses, app_keywords, 'Aplicaciones M√©dicas')
        else:
            # Pregunta 3: Usualmente sobre limitaciones/futuro
            limit_keywords = ['limitation', 'challenge', 'future', 'recommendation', 'improvement', 'limitaci√≥n', 'desaf√≠o', 'futuro', 'recomendaci√≥n', 'mejora']
            categories = self._categorize_by_keywords(responses, limit_keywords, 'Limitaciones y Futuro')
        
        return {
            'patterns': list(word_freq.most_common(5)),
            'categories': categories,
            'frequencies': {cat: len(items) for cat, items in categories.items()},
            'keywords': list(word_freq.most_common(10))
        }

    def _categorize_by_keywords(self, responses, keywords, default_category):
        """Categoriza respuestas bas√°ndose en palabras clave."""
        categories = {default_category: []}
        other_category = 'Otros aspectos'
        categories[other_category] = []
        
        for response in responses:
            response_lower = response.lower()
            if any(keyword in response_lower for keyword in keywords):
                categories[default_category].append(response)
            else:
                categories[other_category].append(response)
        
        # Eliminar categor√≠as vac√≠as
        return {cat: items for cat, items in categories.items() if items}
    
    def _integrate_visualizations(self, visualizations_data, sms_data):
        """Integrar las visualizaciones existentes del sistema en el reporte"""
        story = []
                
        # Integrar an√°lisis sem√°ntico
        if 'semantic_analysis' in visualizations_data:
            semantic_image = self._base64_to_reportlab_image(
                visualizations_data['semantic_analysis']['image_base64']
            )
            if semantic_image:
                story.append(semantic_image)
                story.append(Paragraph("Figure 2: Semantic Analysis Distribution", self.styles['Normal']))
        
        # Integrar gr√°fico de burbujas
        if 'bubble_chart' in visualizations_data:            
            bubble_image = self._base64_to_reportlab_image(
                visualizations_data['bubble_chart']['image_base64']
            )
            if bubble_image:
                story.append(bubble_image)
                story.append(Paragraph("Figure 3: Techniques vs Applications Bubble Chart", self.styles['Normal']))

        
        return story
    
    def _generate_information_extraction_table(self, articles_data):
        """Generar una tabla separada de extracci√≥n de informaci√≥n para cada art√≠culo"""
        story = []
        
        # T√≠tulo de la secci√≥n
        story.append(Paragraph("D. INFORMATION EXTRACTION", self.styles['CustomHeading2']))
        story.append(Spacer(1, 10))
        
        # Filtrar solo art√≠culos seleccionados
        selected_articles = [a for a in articles_data if a.get('estado') == 'SELECTED']
        
        if not selected_articles:
            story.append(Paragraph("No selected articles found for extraction.", self.styles['CustomNormal']))
            return story
        
        # Generar UNA tabla por cada art√≠culo
        for idx, article in enumerate(selected_articles, 1):
            
            # Crear datos para ESTA tabla espec√≠fica (solo 2 columnas)
            # IMPORTANTE: Usar Paragraph para textos largos que necesitan ajustarse
            table_data = []
            
            # Fila 1: Encabezado principal
            table_data.append([
                Paragraph("extraccion de informacion", self.styles['CustomHeading2']), 
                ""
            ])
            
            # Fila 2: T√≠tulo
            titulo = article.get('titulo', 'N/A')
            table_data.append([
                Paragraph("Titulo", self.styles['CustomHeading5']), 
                Paragraph(titulo, self.styles['CustomNormal'])
            ])
            
            # Fila 3: Autor
            autor = article.get('autores', 'N/A')
            table_data.append([
                Paragraph("Autor", self.styles['CustomHeading5']), 
                Paragraph(autor, self.styles['CustomNormal'])
            ])
            
            # Fila 4: Publicaci√≥n
            publicacion = article.get('journal', 'N/A')
            table_data.append([
                Paragraph("Publicacion", self.styles['CustomHeading5']), 
                Paragraph(publicacion, self.styles['CustomNormal'])
            ])
            
            # Fila 5: A√±o
            a√±o = str(article.get('anio_publicacion', 'N/A'))
            table_data.append([
                Paragraph("A√±o", self.styles['CustomHeading5']), 
                Paragraph(a√±o, self.styles['CustomNormal'])
            ])
            
            # Fila 6: sub_pregunta1
            subq1 = article.get('respuesta_subpregunta_1', 'Sin respuesta')
            table_data.append([
                Paragraph("sub_pregunta1", self.styles['CustomHeading5']), 
                Paragraph(subq1, self.styles['CustomNormal'])
            ])
            
            # Fila 7: sub_pregunta2
            subq2 = article.get('respuesta_subpregunta_2', 'Sin respuesta')
            table_data.append([
                Paragraph("sub_pregunta2", self.styles['CustomHeading5']), 
                Paragraph(subq2, self.styles['CustomNormal'])
            ])
            
            # Fila 8: sub_pregunta3
            subq3 = article.get('respuesta_subpregunta_3', 'Sin respuesta')
            table_data.append([
                Paragraph("sub_pregunta3", self.styles['CustomHeading5']), 
                Paragraph(subq3, self.styles['CustomNormal'])
            ])
            
            # Configurar anchos de columna (solo 2 columnas) - M√°s espacio para el contenido
            col_widths = [
                1.3*inch,  # Primera columna para nombres de campos (reducida)
                5.7*inch   # Segunda columna para datos del art√≠culo (aumentada)
            ]
            
            # Crear la tabla para este art√≠culo
            table = Table(table_data, colWidths=col_widths, repeatRows=1)
            
            # Aplicar estilos - Optimizados para texto que se ajusta autom√°ticamente
            table.setStyle(TableStyle([
                # Estilo para el encabezado principal (primera fila)
                ('SPAN', (0, 0), (1, 0)),  # Combinar las 2 columnas de la primera fila
                ('BACKGROUND', (0, 0), (1, 0), colors.lightgrey),
                ('TEXTCOLOR', (0, 0), (1, 0), colors.black),
                ('ALIGN', (0, 0), (1, 0), 'CENTER'),
                ('FONTNAME', (0, 0), (1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (1, 0), 8),
                ('TOPPADDING', (0, 0), (1, 0), 8),
                
                # Estilo para la primera columna (nombres de campos)
                ('BACKGROUND', (0, 1), (0, -1), colors.lightgrey),
                ('TEXTCOLOR', (0, 1), (0, -1), colors.black),
                ('ALIGN', (0, 1), (0, -1), 'LEFT'),
                ('VALIGN', (0, 1), (0, -1), 'TOP'),
                
                # Estilo para la segunda columna (datos del art√≠culo)
                ('BACKGROUND', (1, 1), (1, -1), colors.white),
                ('TEXTCOLOR', (1, 1), (1, -1), colors.black),
                ('ALIGN', (1, 1), (1, -1), 'LEFT'),
                ('VALIGN', (1, 1), (1, -1), 'TOP'),
                
                # Bordes para toda la tabla
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('LINEBELOW', (0, 0), (-1, 0), 2, colors.black),  # L√≠nea m√°s gruesa bajo encabezado
                ('LINEAFTER', (0, 1), (0, -1), 2, colors.black),  # L√≠nea m√°s gruesa despu√©s de nombres de campos
                
                # Padding optimizado para texto que se ajusta
                ('LEFTPADDING', (0, 0), (-1, -1), 8),
                ('RIGHTPADDING', (0, 0), (-1, -1), 8),
                ('TOPPADDING', (0, 1), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 1), (-1, -1), 8),
            ]))
            
            # A√±adir la tabla al documento (SIN espacios entre tablas)
            story.append(Spacer(1, 6))
            story.append(table)  
        return story
    def _generate_analysis_and_discussions_section(self, sms_data, articles_data, visualizations_data=None):
        """
        Genera el apartado '3. Analysis and discussions' usando ChatGPT
        Coloca esta funci√≥n despu√©s de _generate_information_extraction_table
        """
        story = []
        print("üîç Generando apartado 'Analysis and discussions' con IA...")
        
        # PASO 1: Extraer estad√≠sticas detalladas (ya existe en tu c√≥digo)
        stats = self._extract_detailed_statistics(articles_data)
        
        # PASO 2: Crear an√°lisis de patrones por pregunta
        analysis_patterns = self._extract_rsq_patterns_analysis(articles_data, stats)
        
        # PASO 3: Construir el prompt completo para ChatGPT
        prompt = self._build_analysis_discussions_prompt(sms_data, stats, analysis_patterns)
        
        # PASO 4: Generar texto con ChatGPT
        analysis_text = self._generate_ai_text(prompt, max_tokens=2500)
        
        # PASO 5: Procesar y estructurar la respuesta
        story.extend(self._process_analysis_discussions_response(analysis_text))
        
        return story

    def _extract_rsq_patterns_analysis(self, articles_data, stats):
        """
        NUEVO: Extrae patrones espec√≠ficos para cada RSQ
        """
        selected_articles = [a for a in articles_data if a.get('estado') == 'SELECTED']
        
        patterns = {}
        
        # Analizar RSQ1 (m√©todos/t√©cnicas)
        rsq1_responses = [a.get('respuesta_subpregunta_1', '') for a in selected_articles 
                        if a.get('respuesta_subpregunta_1') and a.get('respuesta_subpregunta_1') != 'Sin respuesta disponible']
        patterns['rsq1'] = self._analyze_text_patterns(rsq1_responses, 'techniques')
        
        # Analizar RSQ2 (aplicaciones/dominios)
        rsq2_responses = [a.get('respuesta_subpregunta_2', '') for a in selected_articles 
                        if a.get('respuesta_subpregunta_2') and a.get('respuesta_subpregunta_2') != 'Sin respuesta disponible']
        patterns['rsq2'] = self._analyze_text_patterns(rsq2_responses, 'applications')
        
        # Analizar RSQ3 (limitaciones/futuro)
        rsq3_responses = [a.get('respuesta_subpregunta_3', '') for a in selected_articles 
                        if a.get('respuesta_subpregunta_3') and a.get('respuesta_subpregunta_3') != 'Sin respuesta disponible']
        patterns['rsq3'] = self._analyze_text_patterns(rsq3_responses, 'limitations')
        
        return patterns

    def _analyze_text_patterns(self, responses, category_type):
        """
        NUEVO: Analiza patrones de texto seg√∫n categor√≠a
        """
        if not responses:
            return {"keywords": [], "categories": {}, "summary": "Sin datos suficientes", "total_responses": 0}
        
        # Combinar todas las respuestas
        combined_text = ' '.join(responses).lower()
        
        # Definir palabras clave por categor√≠a
        if category_type == 'techniques':
            key_terms = ['machine learning', 'deep learning', 'algorithm', 'statistical', 'neural network', 
                        'regression', 'classification', 'clustering', 'ai', 'artificial intelligence',
                        'aprendizaje autom√°tico', 'algoritmo', 'estad√≠stico', 'inteligencia artificial']
        elif category_type == 'applications':
            key_terms = ['healthcare', 'medical', 'clinical', 'diagnostic', 'treatment', 'patient',
                        'disease', 'health', 'salud', 'm√©dico', 'cl√≠nico', 'diagn√≥stico', 'tratamiento']
        else:  # limitations
            key_terms = ['limitation', 'challenge', 'problem', 'issue', 'future work', 'improvement',
                        'limitaci√≥n', 'desaf√≠o', 'problema', 'trabajo futuro', 'mejora']
        
        # Contar menciones
        keyword_counts = {}
        for term in key_terms:
            count = combined_text.count(term)
            if count > 0:
                keyword_counts[term] = count
        
        # Crear resumen
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "keywords": top_keywords,
            "total_responses": len(responses),
            "summary": f"Se identificaron {len(keyword_counts)} t√©rminos clave relevantes"
        }

    def _build_analysis_discussions_prompt(self, sms_data, stats, analysis_patterns):
        """
        NUEVO: Construye el prompt espec√≠fico para Analysis and Discussions - VERSI√ìN TEXTO CONTINUO
        """
        prompt = f"""
    Act√∫a como un investigador experto en revisiones sistem√°ticas que necesita redactar el apartado "Analysis and discussions" de un estudio de mapeo sistem√°tico.

    DATOS DE ENTRADA:

    1. Informaci√≥n b√°sica del estudio:
    - Tema de investigaci√≥n: {sms_data.get('titulo_estudio', 'No especificado')}
    - Pregunta principal: {sms_data.get('pregunta_principal', 'No especificada')}
    - Preguntas de investigaci√≥n:
    * RSQ_1: {sms_data.get('subpregunta_1', 'No especificada')}
    * RSQ_2: {sms_data.get('subpregunta_2', 'No especificada')}
    * RSQ_3: {sms_data.get('subpregunta_3', 'No especificada')}
    - N√∫mero total de estudios seleccionados: {stats['selected_count']} art√≠culos
    - Per√≠odo de b√∫squeda: {sms_data.get('anio_inicio', 'N/A')} - {sms_data.get('anio_final', 'N/A')}
    - Bases de datos utilizadas: Scopus, Web of Science, PubMed
    - Tasa de selecci√≥n: {stats['selection_rate']}%

    2. Resultados cuantitativos principales:
    - Distribuci√≥n temporal: {stats['year_distribution']}
    - A√±o m√°s productivo: {stats['most_productive_year'][0]} con {stats['most_productive_year'][1]} art√≠culos
    - Distribuci√≥n por revista (top 5): {dict(list(stats['journal_distribution'].items())[:5])}
    - Distribuci√≥n por enfoque: {stats['enfoque_distribution']}

    3. An√°lisis de respuestas por pregunta:
    RSQ_1 - {analysis_patterns['rsq1']['total_responses']} respuestas analizadas:
    Principales t√©rminos: {analysis_patterns['rsq1']['keywords']}

    RSQ_2 - {analysis_patterns['rsq2']['total_responses']} respuestas analizadas:
    Principales t√©rminos: {analysis_patterns['rsq2']['keywords']}

    RSQ_3 - {analysis_patterns['rsq3']['total_responses']} respuestas analizadas:
    Principales t√©rminos: {analysis_patterns['rsq3']['keywords']}

    4. Figuras y gr√°ficos disponibles:
    - Figura 1: Diagrama PRISMA (proceso de selecci√≥n)
    - Figura 2: An√°lisis sem√°ntico (distribuci√≥n de estudios por clusters)
    - Figura 3: Gr√°fico de burbujas (t√©cnicas vs aplicaciones)
    - Tabla de extracci√≥n: {stats['selected_count']} art√≠culos con informaci√≥n detallada

    INSTRUCCIONES PARA EL TEXTO:

    Redacta un texto acad√©mico continuo y fluido que aborde TODOS estos aspectos en el siguiente orden l√≥gico:

    1. P√°rrafo introductorio: Explica que el an√°lisis de {stats['selected_count']} estudios seleccionados muestra la distribuci√≥n por categor√≠as, c√≥mo esto permite identificar √°reas enfatizadas y futuras l√≠neas de investigaci√≥n. Conecta con el mapa sistem√°tico desarrollado en las figuras 2 y 3, y hace referencia a las m√∫ltiples √°reas cubiertas en el per√≠odo {sms_data.get('anio_inicio', 'N/A')}-{sms_data.get('anio_final', 'N/A')}.

    2. An√°lisis temporal y cuantitativo: Presenta la distribuci√≥n temporal "{stats['year_distribution']}", identifica el a√±o m√°s productivo ({stats['most_productive_year'][0]} con {stats['most_productive_year'][1]} estudios), analiza tendencias y proporciona estad√≠sticas sobre concentraci√≥n de publicaciones.

    3. Interpretaci√≥n de figuras: Analiza la Figura 2 (an√°lisis sem√°ntico) describiendo clusters, explica la Figura 3 (gr√°fico de burbujas) mostrando relaciones entre t√©cnicas y aplicaciones, identifica conjuntos m√°s grandes de contribuciones.

    4. An√°lisis por pregunta de investigaci√≥n: Para RSQ_1 resume los {analysis_patterns['rsq1']['total_responses']} estudios identificando enfoques metodol√≥gicos frecuentes. Para RSQ_2 analiza los {analysis_patterns['rsq2']['total_responses']} estudios relevantes identificando aplicaciones m√°s exploradas. Para RSQ_3 examina los {analysis_patterns['rsq3']['total_responses']} estudios identificando limitaciones reportadas.

    5. Gaps y oportunidades: Se√±ala √°reas con pocas investigaciones bas√°ndose en distribuci√≥n cuantitativa, identifica limitaciones en enfoques actuales, menciona aplicaciones sub-exploradas.

    6. T√©cnicas e innovaciones: Resalta t√©cnicas avanzadas encontradas (IA, ML, estad√≠stica), menciona enfoques √∫nicos, conecta t√©cnicas con aplicaciones espec√≠ficas.

    7. S√≠ntesis final: Resume hallazgos principales que responden a la pregunta principal, conecta resultados con las tres RSQs, menciona implicaciones pr√°cticas y sugiere l√≠neas futuras.

    ESTILO DE REDACCI√ìN:
    - Tono acad√©mico formal pero accesible
    - Incluye referencias espec√≠ficas a "Figura 2", "Figura 3", "Tabla de extracci√≥n"
    - Usa conectores l√≥gicos entre ideas para mantener fluidez
    - Equilibrio entre descripci√≥n cuantitativa y an√°lisis cr√≠tico
    - Respalda afirmaciones con datos cuantitativos espec√≠ficos
    - Evita gerundios, usa espa√±ol acad√©mico formal
    - Usa n√∫meros exactos proporcionados, no aproximaciones
    - Longitud total: aproximadamente 1000-1500 palabras

    FORMATO DE RESPUESTA:
    Responde con un texto continuo estructurado en p√°rrafos (sin subt√≠tulos numerados), que fluya naturalmente de un tema al siguiente. Cada p√°rrafo debe tener 100-200 palabras. Separa p√°rrafos con doble salto de l√≠nea.

    No incluyas explicaciones adicionales, solo el texto acad√©mico del apartado.
    """
        
        return prompt

    def _process_analysis_discussions_response(self, analysis_text):
        """
        NUEVO: Procesa la respuesta de ChatGPT y la convierte en elementos ReportLab
        Mantiene el texto como flujo continuo sin dividir en subsecciones
        """
        import re
        story = []
        
        # Agregar t√≠tulo de la secci√≥n
        story.append(Paragraph("3. Analysis and discussions", self.styles['CustomHeading2']))
        story.append(Spacer(1, 15))
        
        # Dividir el texto en p√°rrafos por saltos de l√≠nea dobles
        paragraphs = analysis_text.split('\n\n')
        
        for paragraph in paragraphs:
            # Limpiar el p√°rrafo
            clean_paragraph = paragraph.strip()
            
            # Si el p√°rrafo no est√° vac√≠o
            if clean_paragraph:
                # Remover posibles numeraciones autom√°ticas (3.1, 3.2, etc.) si las hay
                clean_paragraph = re.sub(r'^3\.\d+\s*', '', clean_paragraph)
                
                # Remover saltos de l√≠nea internos del p√°rrafo
                clean_paragraph = clean_paragraph.replace('\n', ' ')
                
                # Agregar como p√°rrafo normal
                story.append(Paragraph(clean_paragraph, self.styles['CustomNormal']))
                story.append(Spacer(1, 12))
        
        return story
    def _generate_references(self, articles_data):
        """Generar referencias bibliogr√°ficas en formato APA"""
        story = []
        
        story.append(Paragraph("REFERENCES", self.styles['CustomHeading2']))
        
        selected_articles = [a for a in articles_data if a.get('estado') == 'SELECTED']
        
        for i, article in enumerate(selected_articles[:30], 1):  # Limitar referencias
            # Formato APA simplificado
            authors = article.get('autores', 'Unknown authors')
            year = article.get('anio_publicacion', 'n.d.')
            title = article.get('titulo', 'Untitled')
            journal = article.get('journal', 'Unknown journal')
            
            reference = f"[{i}] {authors} ({year}). {title}. {journal}."
            story.append(Paragraph(reference, self.styles['Normal']))
            story.append(Spacer(1, 6))
        
        return story
    
    def _generate_conclusions(self, sms_data, articles_data):
        """Generar conclusiones usando IA"""
        story = []
        
        story.append(Paragraph("CONCLUSIONS", self.styles['CustomHeading2']))
        
        # Generar conclusiones con IA
        conclusions_prompt = (
            f"Escribe 3 conclusiones para una revisi√≥n sistem√°tica sobre {sms_data['titulo_estudio']}. "
            f"Basado en el an√°lisis de {len(articles_data)} estudios. Incluye implicaciones para futuras investigaciones."
            f" Evita el uso de gerundios. Escribe en espa√±ol, con coherencia y profundidad."
            f"solo dame el texto sin titulo ni listas."
            f"Responde SOLO con el p√°rrafo, sin texto adicional."
        )
        
        conclusions_text = self._generate_ai_text(conclusions_prompt)
        story.append(Paragraph(conclusions_text, self.styles['CustomNormal']))
        
        return story
    
    def _extract_keywords_from_title(self, title):
        """Extraer keywords del t√≠tulo"""
        # Implementaci√≥n simple - en producci√≥n usar NLP m√°s avanzado
        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = [word.lower() for word in title.split() if word.lower() not in stop_words and len(word) > 3]
        return ', '.join(words[:6])  # M√°ximo 6 keywords
    
    def _base64_to_reportlab_image(self, base64_string, max_width=6*inch, max_height=4*inch):
        """Convertir imagen base64 a formato ReportLab"""
        try:
            # Decodificar base64
            image_data = base64.b64decode(base64_string)
            image_buffer = io.BytesIO(image_data)
            
            # Crear imagen ReportLab
            image = RLImage(image_buffer, width=max_width, height=max_height)
            return image
        except Exception as e:
            print(f"Error converting base64 to image: {e}")
            return None